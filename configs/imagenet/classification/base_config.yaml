# ---------------------------- TRAINER -------------------------------------------
trainer:
  max_epochs: 600
  data_type: "float32"
  checkpoint_path: "../../checkpoint/imagenet/classification/base"
  checkpoint_filename: "multi_last"
  checkpoint_filename_for_loading: "multi_last_odd" #multi_last_odd or multi_last_even

  resume_from_checkpoint: False

# ---------------------------- PARALLELISM -------------------------------------
parallelism:
  fsdp_size: 1
  simple_ddp_size: 8
  tensor_par_size: 1
  seq_par_size: 1

# ---------------------------- MODEL -------------------------------------------
model:
  lr: 0.0001
  beta_1: 0.9
  beta_2: 0.95
  weight_decay: 1e-5
  warmup_steps: 1000
  max_steps: 20000
  warmup_start_lr: 1e-8
  eta_min: 1e-8

  net:
    init_args:
      default_vars: [
        "red",
        "green",
        "blue",
      ]
      tile_size: [256, 256]
      patch_size: 16
      embed_dim: 768
      depth: 12
      num_heads: 12
      mlp_ratio: 4
      drop_path: 0.0
      drop_rate: 0.0
      twoD: True
      use_varemb: False
      adaptive_patching: True
      fixed_length: 196
      separate_channels: False
      use_adaptive_pos_emb: True

# ---------------------------- DATA -------------------------------------------
data:
  dataset: "imagenet"
  dict_root_dirs: {
    'imagenet': '/lustre/orion/nro108/world-shared/enzhi/dataset/imagenet/train',
  }
  dict_start_idx: {
    'imagenet': 0,
  }
  dict_end_idx: {
    'imagenet': 1,
  }
  dict_buffer_sizes: {
    'imagenet': 100,
  }
  num_channels_used: {
    'imagenet': 3,
  }
  dict_in_variables: {
    'imagenet': [
     "red", 
     "green", 
     "blue", 
    ],
  }
  batch_size: 32
  num_workers: 1
  pin_memory: False
  single_channel: False
  tile_overlap: 0.0
  use_all_data: False
  num_classes: 1000

# ---------------------------- DATASET SPECIFIC OPTIONS -------------------------------------------
dataset_options:

  imagenet_resize: {
      'imagenet': [256,256],
    }

# ---------------------------- QUANTIZATION -------------------------------------------
quantization:
  enabled: false  # Set to true to enable quantization
  method: "qat"   # qat (Quantization Aware Training) or ptq (Post Training Quantization)
  backend: "qnnpack"  # qnnpack for AMD/ROCm, fbgemm for NVIDIA
  bits: 8         # 8, 4, or 1 bit quantization
  calibration_samples: 1000  # Number of samples for calibration (PTQ only)
  quantize_layers: ["linear", "conv"]  # Layer types to quantize
  exclude_layers: ["cls_token", "pos_embed"]  # Layers to exclude from quantization
  
  # Frontier supercomputer specific optimizations
  rocm_optimizations: true
  mi250x_kernels: true
  
  # QAT specific settings
  qat:
    fake_quantize: true
    observer_type: "minmax"  # minmax, moving_average_minmax
    quant_min: 0
    quant_max: 255  # For 8-bit: 0-255, 4-bit: 0-15, 1-bit: 0-1
    
  # Performance tuning for Gordon Bell submission
  performance_mode: "maximum"  # enables all optimizations
  profile_quantization: true  # detailed performance profiling

# ---------------------------- LOAD BALANCING -------------------------------------------
load_balancing:
  auto_load_balancing: True
  batches_per_rank_epoch: {
    'imagenet': 4935,
  }
  dataset_group_list: '1:1:1:1:1:1:1:1'
